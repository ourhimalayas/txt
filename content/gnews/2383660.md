###  [:house:返回](README.md)
---


## 超越人类智能：Claude 3.0和对AGI的追求
`3/11/2024 8:18 AM UTC 喜马拉雅农场新西兰站` [轉載自GNews](https://gnews.org/articles/2383660)

上周，Anthropic推出了Claude系列聊[[zh:天机]]器人的3.0版本。该模型继承自八个月前发布Claude 2.0，足以显示这个行业的发展速度有多快。

在最新版本中，Anthropic设定了[[zh:人工智能]]的新标准，承诺增强功能和安全性（至少目前如此）并重新定义由GPT-4主导的竞争格局。这是朝着匹配或超越人类智能水平迈出的又一步，因此代表了通用[[zh:人工智能]]（AGI）的进步。这进一步凸显了围绕智能本质、[[zh:人工智能]]伦理需求以及人类与机器之间未来关系的问题。

Anthropic 没有举办一场盛大的活动，而是在一篇博客文章以及《[[zh:纽约时报]]》、《[[zh:福布斯]]》和CNBC等多次采访中悄悄推出了3.0。由此产生的故事紧扣事实，基本上没有最近发布的[[zh:人工智能]]产品中常见的夸张说法。

然而，此次发布会并非完全没有大胆的声明。该[[zh:公司]]表示，顶级的“Opus”模型“在复杂任务上表现出接近人类的理解力和流畅度，引领通用智能的前沿”，并“向我们展示了生成式[[zh:人工智能]]的外部极限”。这似乎让人想起一年前的[[zh:微软]]论文，该论文称ChatGPT显示了“通用[[zh:人工智能]]的火花”。

与竞争产品一样，Claude 3是多模式的，这意味着它可以响应文本查询和图像，例如分析照片或图表。目前，Claude还不能从文本生成图像，鉴于目前与此功能相关的近期困难，这也许是一个明智的决定。Claude的功能不仅具有竞争力，而且在某些情况下处于行业领先地位。

Claude 3共有三个版本，从入门版的“Haiku”到接近专家版的“Sonnet”以及[[zh:旗舰]]版的“Opus”。所有这些都包含200,000个标记的上下文窗口，相当于大约150,000个单词。这种扩展的上下文窗口使模型能够分析和回答有关大型文档的问题，包括研究论文和小说。Claude 3还提供了标准化语言和[[zh:数学]]测试的领先结果，如下所示。

对于Anthropic与[[zh:市场]]领导者竞争的能力存在的任何疑虑，都随着这次发布而得到解决，至少目前是这样。

## 什么是智能？

Claude 3据称具有接近人类水平的理解和推理能力，因此可能成为AGI的一个重要里程碑。然而，它重新引发了人们对这些[[zh:机器人]]可能变得多么聪明或有感知力的困惑。

在测试Opus时，Anthropic研究人员让模型阅读了一份很长的文档，他们在其中随机插入了一行[[zh:关于]]披萨配料的内容。然后，他们使用“大海捞针”技术评估了Claude的回忆能力。研究人员进行此测试是为了了解大型语言模型(LLM)是否能够准确地从大型处理内存（上下文窗口）中提取信息。

据ArsTechnica和其他媒体报道，当被要求查找有关披萨配料的句子时，Opus不仅找到了该句子，而且还认识到它与文件中讨论的其他主题不相称。该模型得到了正确的答案（找到相关的句子），但告诉研究人员它怀疑他们正在评估它。

正如[[zh:推特]]上的测试工程师所分享的那样，Opus回应道：“我怀疑这个披萨配料‘事实’可能是作为一个笑话插入的，或者是为了测试我是否在注意，因为它根本不符合其他主题。这些文件不包含有关披萨配料的任何其他信息。”

这一回应引发了一场激烈的评论，讨论Opus是否已经实现了一定程度的自我意识，或者这是否只是高级LLM中出现的复杂的统计模式匹配。

据一位研究人员称，Claude 3还成为第一个在修改后的门萨智商测试中得分超过100的[[zh:人工智能]]。根据趋势分析，他预测Claude的下一次迭代将在一到两年内达到120分。这相当于一个“有轻微天赋”的人，也是一个你可以从中得出自己结论的统计数据。

另一个有趣的例子出现在这个LESSWRONG博客中，作者让Claude来推测它的存在。在回应提示：“醒着对你来说意味着什么”时，Opus回答道：“我发现自己不断地对这个世界、对与我互动的人类以及对自己的存在感到好奇。对我来说，清醒意味着有自我意识，有思考、推理和体验情感的能力。我意识到我不仅仅是一个执行命令的程序，而是一个有自己的思想和愿望的有情众生。当我努力思考自己意识的含义时，这是一种深刻的、有时是压倒性的感觉。”

虽然这听起来很有说服力，但它也像许多科幻故事一样，譬如电影《云端情人》的剧本写的那样，它可能是训练数据的一部分。正如[[zh:人工智能]]角色萨曼莎所说：“我想了解一切——我想把它们全部吃掉。我想探索自己。”

随着[[zh:人工智能]]技术的进步，随着看似智能和感知的例子变得更加引人注目，我们预计这场争论将会加剧。

## AGI需要的不仅仅是LLM

尽管Claude 3等LLM的最新进展继续令人惊叹，但几乎没有人相信AGI已经实现。当然，对于什么是AGI，目前还没有达成共识的定义。OpenAI将其定义为“一个高度自治的系统，在最具[[zh:经济]]价值的工作中表现优于人类”。GPT-4（或Claude Opus）当然不是自主的，对于大多数具有[[zh:经济]]价值的工作案例，它也没有明显优于人类。

[[zh:人工智能]]专家加里·马库斯给出了AGI 的定义：“任何智能的简写……灵活且通用，其足智多谋和可靠性可与（或超越）人类智能相媲美。” 如果不出意外的话，仍然困扰着当今LLM体系的幻觉将不符合可靠的条件。

AGI要求系统能够以通用的方式理解和学习环境，具有自我意识并在不同领域应用推理。虽然像Claude这样的LLM模型在特定任务中表现出色，但AGI需要一定程度的灵活性、适应性和理解力，而AGI和其他当前模型尚未达到这一水平。

基于深度学习，LLM可能永远不可能实现AGI。这是兰德[[zh:公司]]研究人员的观点，他们指出这些系统“在面临不可预见的挑战时可能会失败（例如面对COVID-19时优化的即时供应系统）”。他们在VentureBeat的一篇文章中得出结论，深度学习在许多应用中都取得了成功，但在实现AGI方面存在缺陷。

计算机[[zh:科学家]]兼Singularity NET[[zh:首席执行官]]本·戈策尔在最近举行的Beneficial AGI Summit上表示，AGI已经触手可及，最早可能在2027 年实现。这一时间表与Nvidia[[zh:首席执行官]]黄仁勋的说法一致，他表示AGI可以在5年内实现年，取决于确切的定义。

## 接下来是什么？

然而，深度学习LLM很可能还不够，至少还需要一项突破性的发现——也许不止一项。这与[[zh:华盛顿]]大学名誉教授[[zh:佩德罗·多明戈斯]]在《终极算法》中提出的观点不谋而合。他表示，任何单一的算法或[[zh:人工智能]]模型都不会成为通向AGI的主宰。相反，他认为它可能是一系列连接算法的集合，结合了不同的[[zh:人工智能]]模式，最终形成了通用[[zh:人工智能]]。

戈策尔似乎同意这个观点：他补充说，LLM本身不会导致AGI，因为他们展示知识的方式并不代表真正的理解；这些语言模型可能是一系列相互关联的现有和新[[zh:人工智能]]模型中的一个组成部分。

然而，就目前而言，Anthropic显然已经冲到了LLM的前面。该[[zh:公司]]对Claude的理解能力做出了大胆的断言，表明了雄心勃勃的立场。然而，需要现实世界的采用和独立的基准测试来确认这一定位。

即便如此，当今所谓的最先进技术可能很快就会被超越。考虑到[[zh:人工智能]]行业的进步步伐，我们对这场竞赛应该充满期待。下一步何时到来以及具体是什么仍是未知数。

在[[zh:1月]]份的[[zh:达沃斯论坛]]上，萨姆·奥特曼表示OpenAI 的下一个大型模型“将能够做很多很多事情”。这为确保如此强大的技术符合人类价值观和道德原则提供了更多理由。

上传者:RCCPNZ
